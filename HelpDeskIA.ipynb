{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzTaHtjYbK9OyHKdf8elob",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lokolope24/Imersao_IA_Alura/blob/main/HelpDeskIA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bibliotecas\n",
        "\n",
        "Instalando o Gradio e Importando as demais bibliotecas para usar no projeto\n"
      ],
      "metadata": {
        "id": "c1CG6e3lMCU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U -q gradio"
      ],
      "metadata": {
        "id": "-KqLColk5_Ut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "416ff5b2-0c0f-4393-d505-6c821266c592"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.5/315.5 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "import numpy as np\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "iN5_YTolLNp-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# API Key e Modelo Gemini\n",
        "\n",
        "Utilizando a API Key do Google IA Studio e o Modelo de LLM Gemini (gemini-1.0-pro), abaixo está a configuração do modelo generativo e do embedding:\n"
      ],
      "metadata": {
        "id": "5q9G6zzTNGzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = userdata.get(\"secret_key\")\n",
        "GOOGLE_API_KEY=api_key\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "uVtrO6ZoA2Y9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = {\n",
        "    \"candidate_count\" : 1,\n",
        "    \"temperature\": 0.5,\n",
        "}\n",
        "\n",
        "safety_settings = {\n",
        "    \"HARASSMENT\" : \"BLOCK_NONE\",\n",
        "    \"HATE\": \"BLOCK_NONE\",\n",
        "    \"SEXUAL\":\"BLOCK_NONE\",\n",
        "    \"DANGEROUS\" : \"BLOCK_NONE\",\n",
        "}\n",
        "\n",
        "model_generative_AI = genai.GenerativeModel('gemini-1.0-pro', generation_config=generation_config, safety_settings=safety_settings)"
      ],
      "metadata": {
        "id": "Ss3kB1OHuuYB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Modelo de Embedding utilizado:\n",
        "model_embedding = \"models/embedding-001\""
      ],
      "metadata": {
        "id": "TIcQwbhQurhf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importando Dados\n",
        "\n",
        "Importando o arquivo excel que iremos usar como a base de dados do projeto HelpDesk e realizando o pré processamento dos dados.\n",
        "\n",
        "  O arquivo contém um histórico de atendimento dentro da área da TI voltado a infraestrutura, conténdo como pontos focais da análise as colunas \"Assunto\", \"Diagnóstico\", \"Causa\" e \"Ação\".\n",
        "  Realizamos também um pré-processamento dos dados e limpando as possiveis sujeiras que estão no arquivo.\n",
        "  "
      ],
      "metadata": {
        "id": "YJRbEL0aMWK3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vmx6Rd1q5o2P"
      },
      "outputs": [],
      "source": [
        "# Lê o arquivo Excel usando pandas\n",
        "df_helpdesk = pd.read_excel('helpdesk.xlsx')\n",
        "\n",
        "#Renomeia as colunas na ordem que foram importadas:\n",
        "df_helpdesk.columns = [\"assunto\", \"diagnostico\", \"causa\", \"acao\"]\n",
        "\n",
        "# Remove registros onde qualquer uma das colunas especificadas tem um valor nulo\n",
        "df_helpdesk = df_helpdesk.dropna(subset=[\"assunto\", \"diagnostico\", \"causa\", \"acao\"])\n",
        "\n",
        "#Unificando as colunas em apenas uma.\n",
        "df_helpdesk[\"diagnostico_causa_acao\"] = \"Diagnostico : \"+ df_helpdesk[\"diagnostico\"] + \" Causa : \" + df_helpdesk[\"causa\"] + \" Acao : \" + df_helpdesk[\"acao\"]\n",
        "\n",
        "#deletando as demais colunas para organização\n",
        "df_helpdesk = df_helpdesk.drop(columns=[\"diagnostico\", \"causa\", \"acao\"])\n",
        "\n",
        "df_helpdesk = df_helpdesk.head(100)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding\n",
        "\n",
        "\n",
        "1.    Criando a função de embedding do conteúdo do DataFrame\n",
        "2.    Criando o embedding da consulta para comparar com o conteúdo do DataFrame\n",
        "\n"
      ],
      "metadata": {
        "id": "KJ_07qecN2x4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embedding_function(title, text):\n",
        "\n",
        "  return genai.embed_content(model=model_embedding, content=text, title=title, task_type=\"RETRIEVAL_DOCUMENT\")[\"embedding\"]"
      ],
      "metadata": {
        "id": "awix5IhON2RW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_helpdesk[\"embeddings\"] = df_helpdesk.apply(lambda row:\n",
        "                                              embedding_function(row[\"assunto\"],\n",
        "                                                                 row[\"diagnostico_causa_acao\"]), axis=1)"
      ],
      "metadata": {
        "id": "E0IqR89WOT8w"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gerar_consulta(consulta, base, model):\n",
        "\n",
        "  #Gera o embedding da consulta:\n",
        "  embedding_da_consulta = genai.embed_content(model=model_embedding, content=consulta, task_type=\"RETRIEVAL_QUERY\")[\"embedding\"]\n",
        "\n",
        "  produtos_escalares = np.dot(np.stack(df_helpdesk[\"embeddings\"]), embedding_da_consulta)\n",
        "\n",
        "  indice = np.argmax(produtos_escalares)\n",
        "\n",
        "  return df_helpdesk.iloc[indice][\"diagnostico_causa_acao\"]"
      ],
      "metadata": {
        "id": "ILg36DTio5_F"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Função Interativa\n",
        "\n",
        "Criando uma função para utilizar o modelo de Gemini + embeddings\n"
      ],
      "metadata": {
        "id": "CoUnTmB8867T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def consulta_interativa(consulta):\n",
        "    trecho = gerar_consulta(consulta, df_helpdesk, model_embedding)\n",
        "    prompt = f\"Responda como um especialista em Infraestrutura que está auxiliando o funcionário novo que está com a seguinte dúvida: {consulta} tendo como informações a seguinte base de referência {trecho}\"\n",
        "    response = model_generative_AI.generate_content(prompt)\n",
        "    return response.text\n"
      ],
      "metadata": {
        "id": "FkfEH68zLU_4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Interface gráfica (Gradio)\n",
        "\n",
        "utilizando o Gradio para criar o chatbot e ter o prompt de perguntas e respostas."
      ],
      "metadata": {
        "id": "sE_mu9Av9SOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interface = gr.Interface(fn=consulta_interativa,\n",
        "                         inputs=\"text\",\n",
        "                         outputs=\"text\",\n",
        "                         title=\"HelpDesk IAssist\",\n",
        "                         description=\"Diga, em que posso lhe auxiliar?\")\n",
        "interface.launch(debug=True, share=True)"
      ],
      "metadata": {
        "id": "lw1bSLvM2IDI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "c0ea61a6-828c-4ced-eba8-ce9f63719f88"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://f1dc584331215ed5ce.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f1dc584331215ed5ce.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://f1dc584331215ed5ce.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}